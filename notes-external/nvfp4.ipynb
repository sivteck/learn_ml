{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bade7b0e-7781-4551-8b80-37a50ee7b0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Quantized tensor:\n",
      "tensor([[185.,  81.,  58.,  ...,  30., 179.,  21.],\n",
      "        [232., 187., 173.,  ..., 205.,  73., 100.],\n",
      "        [239., 238.,  44.,  ...,  13., 202., 123.],\n",
      "        ...,\n",
      "        [162., 119., 211.,  ...,  82., 139., 125.],\n",
      "        [123., 144., 187.,  ...,  13., 100., 186.],\n",
      "        [ 18., 174., 211.,  ...,   7., 158., 204.]],\n",
      "       dtype=torch.float4_e2m1fn_x2)\n",
      "torch.Size([512, 64])\n",
      ">> Blockwise scales\n",
      "tensor([[320., 384., 256.,  ..., 240., 240., 320.],\n",
      "        [288., 224., 240.,  ..., 352., 256., 320.],\n",
      "        [144., 288., 288.,  ..., 256., 352., 256.],\n",
      "        ...,\n",
      "        [208., 320., 256.,  ..., 240., 192., 240.],\n",
      "        [208., 192., 320.,  ..., 384., 240., 288.],\n",
      "        [288., 352., 288.,  ..., 416., 256., 256.]], dtype=torch.float8_e4m3fn)\n",
      "torch.Size([512, 8])\n",
      ">> Global scale:\n",
      "tensor(80956.2344)\n"
     ]
    }
   ],
   "source": [
    "# From https://x.com/maharshii/status/1985002945306583140\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "FP8_AMAX = 448.0\n",
    "FP8_DTYPE = torch.float8_e4m3fn\n",
    "\n",
    "FP4_AMAX = 6.0\n",
    "FP4_DTYPE = getattr(torch, \"float4_e2m1fn_x2\", torch.uint8)\n",
    "# midpoints and the corresponding bins\n",
    "# representable positives = [0.0, 0.5, 1.0, 1.5, 2.0, 3.0, 4.0, 6.0]\n",
    "thresholds = [\n",
    "    (5.0, 0b0110), (3.5, 0b0101), (2.5, 0b0100),\n",
    "    (1.75, 0b0011), (1.25, 0b0010), (0.75, 0b0001), (0.25, 0b0000),\n",
    "]\n",
    "\n",
    "\n",
    "# x shape: (M, N/16, 16)\n",
    "# - convert each fp32 value into 4 bits along with sign\n",
    "# - pack 8x4bits into 1xint32 value: (M, N/16, 2) i.e. 64 bits\n",
    "# - final view to uint8 (i.e. 2xfp4): (M, N/16, 8) i.e. 64 / 8 \n",
    "def cvt_1xfp32_2xfp4(x: torch.Tensor):\n",
    "    assert x.dtype == torch.float32\n",
    "\n",
    "    bits = x.view(torch.int32)\n",
    "    sign_bit = (bits >> 31) & 0x1\n",
    "\n",
    "    x_abs = x.abs()\n",
    "    # prevent double counting with alternate <= and <\n",
    "    other_bits = torch.full_like(x_abs, 0b0111, dtype=torch.int)\n",
    "    for i, (m, code) in enumerate(thresholds):\n",
    "        mask = x_abs <= m if i % 2 == 0 else x_abs < m\n",
    "        other_bits = torch.where(mask, code, other_bits)\n",
    "\n",
    "    # each fp32 now as e2m1 (pack 8xfp4 values into 1xint32)\n",
    "    e2m1 = (sign_bit << 3) | other_bits\n",
    "\n",
    "    # shape here becomes (M, N/16, 2) as 2x int32\n",
    "    e2m1x2 = (\n",
    "        e2m1[..., ::8]\n",
    "        | (e2m1[..., 1::8] << 4)\n",
    "        | (e2m1[..., 2::8] << 8)\n",
    "        | (e2m1[..., 3::8] << 12)\n",
    "        | (e2m1[..., 4::8] << 16)\n",
    "        | (e2m1[..., 5::8] << 20)\n",
    "        | (e2m1[..., 6::8] << 24)\n",
    "        | (e2m1[..., 7::8] << 28)\n",
    "    )\n",
    "    # shape becomes (M, N/16, 8) after view\n",
    "    # 64 bits / 8 bits, so each element is 2x e2m1\n",
    "    return e2m1x2.view(FP4_DTYPE)\n",
    "\n",
    "\n",
    "# nvfp4 needs two scaling factors\n",
    "# Global encoding scale (dtype: float32):\n",
    "#   s_enc = 6 * 448 / amax_x    -> from calibration\n",
    "#   s_dec = 1 / s_enc\n",
    "# Local encoding scale (per 16-block, dtype: fp8 e4m3):\n",
    "#   s_decb = amax_b / 6\n",
    "#   scales = e4m3(s_decb * s_enc) -> save this\n",
    "#   s_encb = s_enc / scales.float()\n",
    "# Quant:\n",
    "#   xi = q(xi * s_encb)\n",
    "# q here packs 1xfp32 to 8xfp4\n",
    "def quant_nvfp4_torch(x: torch.Tensor, global_scale: torch.Tensor = None):\n",
    "    assert x.shape[-1] % 16 == 0\n",
    "    \n",
    "    batch_dim = tuple(x.shape[:-1])\n",
    "    # (..., N/16, 16)\n",
    "    x_blocks_f32 = x.unflatten(-1, (-1, 16)).float()\n",
    "\n",
    "    q_dtype, q_dtype_max = FP4_DTYPE, FP4_AMAX\n",
    "    s_dtype, s_dtype_max = FP8_DTYPE, FP8_AMAX\n",
    "\n",
    "    if global_scale is None:\n",
    "        global_scale = FP4_AMAX * FP8_AMAX / x_blocks_f32.abs().amax()\n",
    "\n",
    "    # (..., N/16)\n",
    "    s_decb = x_blocks_f32.abs().amax(dim=-1) / q_dtype_max\n",
    "    xs = (s_decb * global_scale).clamp(\n",
    "        -s_dtype_max, s_dtype_max\n",
    "    ).to(s_dtype)\n",
    "\n",
    "    # (..., N/16, 1)\n",
    "    s_encb = (global_scale / xs.float().clip(1e-12)).unsqueeze(-1)\n",
    "    x_blocks_f32 = x_blocks_f32 * s_encb\n",
    "    xq = cvt_1xfp32_2xfp4(x_blocks_f32).reshape(*batch_dim, -1)\n",
    "\n",
    "    return xq, xs, global_scale\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    shape = (512, 128)\n",
    "    x = torch.randn(shape, dtype=torch.bfloat16) * 0.01\n",
    "\n",
    "    xq, xs, global_scale = quant_nvfp4_torch(x)\n",
    "    print(\">> Quantized tensor:\")\n",
    "    print(xq)\n",
    "    print(xq.shape)\n",
    "    print(\">> Blockwise scales\")\n",
    "    print(xs)\n",
    "    print(xs.shape)\n",
    "    print(\">> Global scale:\")\n",
    "    print(global_scale)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
